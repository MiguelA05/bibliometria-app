# üîó ALGORITMO DE UNIFICACI√ìN DE DATOS

## üìä Descripci√≥n General

El algoritmo de unificaci√≥n combina datos de m√∫ltiples fuentes (OpenAlex_General, OpenAlex_Articles, OpenAlex_Conferences) y elimina duplicados usando un sistema de puntuaci√≥n de similitud ponderado.

---

## üîÑ FLUJO DEL ALGORITMO

### 1Ô∏è‚É£ DESCARGAR DE M√öLTIPLES FUENTES

```python
def download_from_sources(sources: List[DataSource]) -> List[ArticleMetadata]:
```

**Proceso:**
- Descarga datos de cada fuente configurada (3 por defecto)
- Marca cada art√≠culo con su `source` (OpenAlex_General, etc.)
- Combina todos los art√≠culos en una lista √∫nica

**Resultado:** Lista de TODOS los art√≠culos (con duplicados potenciales)

---

### 2Ô∏è‚É£ DETECTAR Y ELIMINAR DUPLICADOS

```python
def detect_and_remove_duplicates(articles: List[ArticleMetadata], 
                                 similarity_threshold: float = 0.8)
```

**Algoritmo:**
```python
unique_articles = []
duplicates_log = []

for article in articles:
    is_duplicate = False
    
    # Comparar con art√≠culos ya procesados
    for unique_article in unique_articles:
        similarity = calculate_similarity_score(article, unique_article)
        
        if similarity >= threshold:  # Por defecto: 0.8
            duplicates_log.append(article)
            is_duplicate = True
            break
    
    if not is_duplicate:
        unique_articles.append(article)
```

**Caracter√≠sticas:**
- **Algoritmo:** Comparaci√≥n secuencial (O(n¬≤))
- **Estrategia:** Mantener el primer art√≠culo, marcar los duplicados
- **Orden:** Se mantiene el orden de llegada de los art√≠culos

---

## üéØ C√ÅLCULO DE SIMILITUD (Core del Algoritmo)

### F√≥rmula de Puntuaci√≥n

```python
def calculate_similarity_score(article1, article2) -> float:
    score = 0.0
    
    # 1. T√çTULO (40% peso)
    title_similarity = _calculate_text_similarity(
        article1.title.lower(), 
        article2.title.lower()
    )
    score += title_similarity * 0.4
    
    # 2. DOI (30% peso)
    if article1.doi == article2.doi:
        score += 0.3
    else:
        doi_sim = _normalize_and_compare(article1.doi, article2.doi)
        score += doi_sim * 0.3
    
    # 3. AUTORES (20% peso)
    author_similarity = _calculate_author_similarity(
        article1.authors, 
        article2.authors
    )
    score += author_similarity * 0.2
    
    # 4. A√ëO DE PUBLICACI√ìN (10% peso)
    if article1.year == article2.year:
        score += 0.1
    
    return min(score, 1.0)
```

---

## üìê T√âCNICAS DE SIMILITUD UTILIZADAS

### 1. Similitud de T√≠tulo (40% peso)

**M√©todo:** Jaccard Similarity sobre palabras

```python
def _calculate_text_similarity(text1, text2) -> float:
    words1 = set(text1.split())
    words2 = set(text2.split())
    
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    
    return intersection / union
```

**Ejemplo:**
- Texto 1: "Machine Learning in Healthcare"
- Texto 2: "Machine Learning Applications in Healthcare"
- **Similitud:** 3/6 = 0.5

---

### 2. Similitud de DOI (30% peso)

**Normalizaci√≥n:**
- Remover prefijos: `https://doi.org/`, `doi:`
- Convertir a min√∫sculas
- Comparar strings normalizados

**Ejemplo:**
- DOI 1: "https://doi.org/10.1234/example"
- DOI 2: "doi:10.1234/example"
- **Normalizados:** "10.1234/example"
- **Resultado:** Score completo (0.3)

---

### 3. Similitud de Autores (20% peso)

**M√©todo:** Jaccard Similarity sobre conjunto de autores

```python
def _calculate_author_similarity(authors1, authors2) -> float:
    # Normalizar nombres
    norm1 = [normalize(author) for author in authors1]
    norm2 = [normalize(author) for author in authors2]
    
    # Calcular intersecci√≥n/uni√≥n
    set1, set2 = set(norm1), set(norm2)
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    return intersection / union
```

**Normalizaci√≥n de nombre:**
- Min√∫sculas
- Remover caracteres especiales
- Remover espacios extra

---

### 4. A√±o de Publicaci√≥n (10% peso)

**L√≥gica:**
- Si a√±os coinciden: +0.1
- Si no coinciden: +0.0

---

## ‚öôÔ∏è PAR√ÅMETROS CONFIGURABLES

### Umbral de Similitud (`similarity_threshold`)

**Por defecto:** `0.8`

**Efecto:**
- Umbral m√°s alto (0.9): Solo duplicados muy obvios
- Umbral bajo (0.6): Detecci√≥n agresiva de duplicados

**Ejemplo:**
```python
# Score = 0.85
similarity_threshold = 0.8
# Resultado: ‚úÖ DUPLICADO (0.85 >= 0.8)

# Score = 0.75
similarity_threshold = 0.8
# Resultado: ‚ùå NO DUPLICADO (0.75 < 0.8)
```

---

## üìä EJEMPLO PR√ÅCTICO

### Caso 1: Duplicado Exacto
```
Art√≠culo 1: "Machine Learning in AI"
            DOI: 10.1234/ml
            Autores: ["Smith", "Jones"]
            A√±o: 2023

Art√≠culo 2: "Machine Learning in AI"
            DOI: 10.1234/ml
            Autores: ["Smith", "Jones"]
            A√±o: 2023

C√°lculo:
- T√≠tulo: 1.0 √ó 0.4 = 0.4
- DOI: 1.0 √ó 0.3 = 0.3
- Autores: 1.0 √ó 0.2 = 0.2
- A√±o: 1.0 √ó 0.1 = 0.1
- TOTAL: 1.0

Resultado: ‚úÖ DUPLICADO (score: 1.0 >= 0.8)
```

### Caso 2: Art√≠culo Similar pero Diferente
```
Art√≠culo 1: "Machine Learning in AI" (2023)
Art√≠culo 2: "Deep Learning in AI" (2024)

C√°lculo:
- T√≠tulo: 0.25 √ó 0.4 = 0.1
- DOI: 0.0 √ó 0.3 = 0.0
- Autores: 0.0 √ó 0.2 = 0.0
- A√±o: 0.0 √ó 0.1 = 0.0
- TOTAL: 0.1

Resultado: ‚ùå NO DUPLICADO (score: 0.1 < 0.8)
```

---

## üéØ VENTAJAS DEL ALGORITMO

‚úÖ **Ponderaci√≥n inteligente:** M√°s peso a identificadores √∫nicos (DOI)  
‚úÖ **Flexible:** T√≠tulos con variaciones se detectan  
‚úÖ **Trazable:** Registro completo de duplicados  
‚úÖ **Configurable:** Umbral ajustable seg√∫n necesidades  

---

## ‚ö†Ô∏è LIMITACIONES

‚ùå **Complejidad O(n¬≤):** Cada art√≠culo comparado con todos los anteriores  
‚ùå **Orden dependiente:** El primer art√≠culo siempre se mantiene  
‚ùå **Puede perder variaciones:** Verificaciones exactas solo para DOI  

---

## üí° OPCIONES DE MEJORA

### 1. Optimizaci√≥n de Rendimiento
```python
# Usar hashing para detectar duplicados exactos r√°pidamente
doi_hash = {article.doi for article in processed}
if article.doi in doi_hash:
    # Es probablemente un duplicado
```

### 2. Algoritmo de Agrupaci√≥n
```python
# Usar DBSCAN o similar para clustering
from sklearn.cluster import DBSCAN
clusters = DBSCAN(eps=0.8, min_samples=1).fit(article_vectors)
```

### 3. Embeddings Sem√°nticos
```python
# Usar embeddings de t√≠tulo para similitud sem√°ntica
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
similarity = cosine_similarity(emb1, emb2)
```

---

## üìã RESUMEN

**Algoritmo Actual:**
- **Tipo:** Comparaci√≥n secuencial con similitud ponderada
- **Pesos:** T√≠tulo (40%), DOI (30%), Autores (20%), A√±o (10%)
- **M√©trica:** Jaccard Similarity + Comparaci√≥n exacta
- **Umbral:** 0.8 por defecto
- **Complejidad:** O(n¬≤) en tiempo

**Justificaci√≥n de la Divisi√≥n:**
- ‚úÖ **raw_data/**: Fuente original (necesario para auditabilidad)
- ‚úÖ **unified/**: Resultado del algoritmo (esencial para an√°lisis)
- ‚ö†Ô∏è **duplicates/**: Registro del proceso (√∫til pero opcional)
- ‚úÖ **reports/**: Estad√≠sticas (√∫til para monitoreo)
