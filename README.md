# ğŸ“š BibliometrÃ­a App

Sistema completo de anÃ¡lisis bibliomÃ©trico para la extracciÃ³n, unificaciÃ³n, anÃ¡lisis y visualizaciÃ³n de datos de producciÃ³n cientÃ­fica desde mÃºltiples fuentes acadÃ©micas.

## ğŸ¯ DescripciÃ³n del Proyecto

BibliometrÃ­a App es una aplicaciÃ³n integral diseÃ±ada para automatizar el proceso de recopilaciÃ³n, anÃ¡lisis y visualizaciÃ³n de datos bibliomÃ©tricos. El sistema integra mÃºltiples bases de datos acadÃ©micas (OpenAlex, PubMed, ArXiv), implementa algoritmos avanzados de similitud textual, anÃ¡lisis de frecuencia de palabras, clustering jerÃ¡rquico y generaciÃ³n de visualizaciones interactivas.

### CaracterÃ­sticas Principales

- **ğŸŒ MÃºltiples Fuentes de Datos**: IntegraciÃ³n con OpenAlex, PubMed y ArXiv
- **ğŸ”„ UnificaciÃ³n AutomÃ¡tica**: Proceso automatizado de unificaciÃ³n y eliminaciÃ³n de duplicados
- **ğŸ” AnÃ¡lisis de Similitud Textual**: 6 algoritmos (4 clÃ¡sicos + 2 basados en IA)
- **ğŸ“Š AnÃ¡lisis de Frecuencia**: CÃ¡lculo de frecuencia de palabras y tÃ©rminos asociados
- **ğŸŒ³ Clustering JerÃ¡rquico**: Agrupamiento de abstracts con dendrogramas
- **ğŸ“ˆ Visualizaciones Interactivas**: Mapas de calor, nubes de palabras, lÃ­neas temporales
- **ğŸš€ API REST**: Servidor FastAPI con endpoints para todos los servicios
- **ğŸ’» Interfaz Interactiva**: MenÃº CLI para facilitar el uso

---

## ğŸ“‹ Requerimientos Implementados

El proyecto cumple con 5 requerimientos principales:

### Requerimiento 1: AutomatizaciÃ³n de Descarga y UnificaciÃ³n de Datos

**Funcionalidad**: Proceso automatizado de descarga de informaciÃ³n desde mÃºltiples bases de datos acadÃ©micas, unificaciÃ³n en un solo archivo y eliminaciÃ³n de duplicados.

**CaracterÃ­sticas**:
- âœ… Descarga automÃ¡tica desde **OpenAlex**, **PubMed** y **ArXiv**
- âœ… UnificaciÃ³n de datos en formato estructurado
- âœ… DetecciÃ³n y eliminaciÃ³n de duplicados mediante algoritmos de similitud
- âœ… GeneraciÃ³n de archivos:
  - `unified/`: Archivo CSV unificado con todos los artÃ­culos Ãºnicos
  - `duplicates/`: Registro de artÃ­culos duplicados eliminados
  - `raw_data/`: Datos crudos por cada fuente
  - `reports/`: Reportes de procesamiento

**Campos incluidos**: TÃ­tulo, autores, abstract, keywords/topics, aÃ±o de publicaciÃ³n, DOI, URL, afiliaciones, paÃ­ses, ciudades, journal, citas, y mÃ¡s.

### Requerimiento 2: AnÃ¡lisis de Similitud Textual

**Funcionalidad**: ImplementaciÃ³n de 6 algoritmos de similitud textual para comparar abstracts de artÃ­culos cientÃ­ficos.

**Algoritmos Implementados**:

1. **Levenshtein (Distancia de EdiciÃ³n)**: Mide la distancia mÃ­nima de ediciÃ³n entre textos
2. **Damerau-Levenshtein**: Extiende Levenshtein incluyendo transposiciones
3. **Jaccard (n-grams)**: Similitud basada en intersecciÃ³n de shingles
4. **TF-IDF Cosine Similarity**: VectorizaciÃ³n estadÃ­stica con importancia de tÃ©rminos
5. **Sentence-BERT**: Embeddings semÃ¡nticos usando transformers
6. **LLM-based (Ollama)**: AnÃ¡lisis semÃ¡ntico profundo con modelos LLM locales

**CaracterÃ­sticas**:
- âœ… ExplicaciÃ³n detallada paso a paso de cada algoritmo
- âœ… AnÃ¡lisis matemÃ¡tico y algorÃ­tmico completo
- âœ… ComparaciÃ³n de 2 o mÃ¡s artÃ­culos simultÃ¡neamente
- âœ… ExtracciÃ³n automÃ¡tica de abstracts desde CSV unificado
- âœ… Resultados con scores, tiempos de procesamiento y detalles tÃ©cnicos

### Requerimiento 3: AnÃ¡lisis de Frecuencia de Palabras

**Funcionalidad**: CÃ¡lculo de frecuencia de apariciÃ³n de palabras de una categorÃ­a especÃ­fica y generaciÃ³n de palabras asociadas.

**CaracterÃ­sticas**:
- âœ… CategorÃ­a predefinida: "Concepts of Generative AI in Education"
- âœ… CÃ¡lculo de frecuencia de apariciÃ³n en abstracts
- âœ… GeneraciÃ³n automÃ¡tica de palabras asociadas (mÃ¡ximo 15)
- âœ… AnÃ¡lisis de precisiÃ³n de palabras asociadas
- âœ… IdentificaciÃ³n de palabras por proximidad contextual

### Requerimiento 4: Agrupamiento JerÃ¡rquico de Abstracts

**Funcionalidad**: ImplementaciÃ³n de clustering jerÃ¡rquico para agrupar abstracts cientÃ­ficos relacionados.

**CaracterÃ­sticas**:
- âœ… 3 mÃ©todos de linkage: Single, Complete, Average
- âœ… Preprocesamiento: VectorizaciÃ³n TF-IDF con normalizaciÃ³n
- âœ… GeneraciÃ³n de dendrogramas en formato PNG
- âœ… EvaluaciÃ³n de calidad de clusters (correlaciÃ³n cophenÃ©tica)
- âœ… DeterminaciÃ³n del mejor algoritmo segÃºn mÃ©tricas
- âœ… Guardado en `results/reports/clustering/`

### Requerimiento 5: AnÃ¡lisis Visual

**Funcionalidad**: GeneraciÃ³n de visualizaciones interactivas y estÃ¡ticas de la producciÃ³n cientÃ­fica.

**Visualizaciones Incluidas**:

1. **Mapa de Calor GeogrÃ¡fico**: DistribuciÃ³n geogrÃ¡fica por paÃ­ses de instituciones (choropleth interactivo)
2. **Nubes de Palabras**: 
   - Abstracts
   - Keywords
   - Combinada
3. **LÃ­nea Temporal**: Publicaciones por aÃ±o y por revista/fuente
4. **ExportaciÃ³n PDF**: Reporte combinado con todas las visualizaciones

**CaracterÃ­sticas**:
- âœ… Visualizaciones interactivas (Plotly) y estÃ¡ticas (Matplotlib)
- âœ… Nubes de palabras dinÃ¡micas que se actualizan con mÃ¡s datos
- âœ… ExportaciÃ³n automÃ¡tica a PDF con formato profesional
- âœ… Guardado en `results/reports/visualizations/`

---

## ğŸš€ InstalaciÃ³n

### Requisitos Previos

- **Python**: 3.8 o superior
- **pip**: Gestor de paquetes de Python
- **Git**: Para clonar el repositorio (opcional)

### Pasos de InstalaciÃ³n

#### 1. Clonar el Repositorio

```bash
git clone <url-del-repositorio>
cd bibliometria-app
```

#### 2. Crear Entorno Virtual (Recomendado)

```bash
# Crear entorno virtual
python3 -m venv venv

# Activar entorno virtual
# Linux/Mac:
source venv/bin/activate

# Windows:
venv\Scripts\activate
```

#### 3. Instalar Dependencias

```bash
# Instalar todas las dependencias
pip install -r requirements.txt

# Si pip no funciona, usa el mÃ³dulo de Python:
python -m pip install -r requirements.txt
# O con Python 3:
python3 -m pip install -r requirements.txt

# Descargar datos de NLTK (OBLIGATORIO para similitud textual)
python -m nltk.downloader punkt stopwords
```

**âš ï¸ Problema con pip?** Si `pip` no estÃ¡ instalado o no se reconoce, consulta la [guÃ­a de soluciÃ³n de problemas](docs/solucion_problemas_pip.md).

**Dependencias Principales**:
- **Framework Web**: FastAPI, Uvicorn
- **Manejo de Datos**: pandas, numpy, requests
- **ValidaciÃ³n**: pydantic, pydantic-settings
- **Similitud Textual**: scikit-learn, nltk, sentence-transformers
- **Clustering**: scipy
- **VisualizaciÃ³n**: matplotlib, wordcloud, plotly, pillow, kaleido

#### 4. Instalar Ollama (Opcional - para algoritmo LLM-based)

Para usar el algoritmo de similitud basado en LLM (Requerimiento 2), necesitas instalar Ollama:

**Linux/Mac:**
```bash
# MÃ©todo automÃ¡tico (recomendado)
bash scripts/install_ollama.sh

# O manualmente
curl -fsSL https://ollama.com/install.sh | sh
ollama serve  # En otra terminal
ollama pull llama3.2:3b  # Descargar modelo
```

**Windows:**
```powershell
# MÃ©todo automÃ¡tico con PowerShell (recomendado)
powershell -ExecutionPolicy Bypass -File scripts/install_ollama.ps1

# O ejecutar el script .bat
scripts\install_ollama.bat

# O manualmente:
# 1. Descargar desde https://ollama.com/download
# 2. Ejecutar OllamaSetup.exe
# 3. Descargar modelo: ollama pull llama3.2:3b
```

**Nota**: Si Ollama no estÃ¡ instalado, el algoritmo LLM-based no estarÃ¡ disponible, pero los otros 5 algoritmos funcionarÃ¡n normalmente.

**Ver guÃ­a detallada para Windows**: Ver [docs/instalacion_ollama_windows.md](docs/instalacion_ollama_windows.md)

#### 5. Configurar Entorno

```bash
# El archivo .env se crea automÃ¡ticamente si no existe
# Puedes personalizar la configuraciÃ³n editando .env
```

#### 6. Verificar InstalaciÃ³n

```bash
# Ejecutar el menÃº principal (verifica dependencias automÃ¡ticamente)
python menu.py
```

---

## ğŸ’» Uso

### MenÃº Interactivo (Recomendado)

El menÃº interactivo es la forma mÃ¡s sencilla de usar todas las funcionalidades del proyecto:

```bash
python menu.py
```

El menÃº principal incluye:

1. **Probar Web Scraping y Generar Resultados** (Requerimiento 1)
   - Configurar consulta de bÃºsqueda
   - Establecer lÃ­mite de artÃ­culos por fuente
   - Configurar umbral de similitud para duplicados
   - Ejecutar proceso completo de automatizaciÃ³n
   - Ver archivos generados

2. **Evaluar Algoritmos de Similitud Textual** (Requerimiento 2)
   - Seleccionar archivo CSV unificado
   - Elegir 2 o mÃ¡s artÃ­culos para comparar
   - Seleccionar algoritmos a ejecutar (todos, clÃ¡sicos, IA, o individual)
   - Ver resultados detallados con explicaciones paso a paso

3. **AnÃ¡lisis de Frecuencia de Palabras** (Requerimiento 3)
   - Seleccionar archivo CSV unificado
   - Configurar categorÃ­a y palabras asociadas
   - Ver frecuencias y palabras asociadas generadas

4. **Agrupamiento JerÃ¡rquico de Abstracts** (Requerimiento 4)
   - Seleccionar archivo CSV unificado
   - Configurar parÃ¡metros de clustering
   - Generar dendrogramas con diferentes mÃ©todos
   - Ver evaluaciÃ³n de calidad de clusters

5. **AnÃ¡lisis Visual** (Requerimiento 5)
   - Seleccionar archivo CSV unificado
   - Generar todas las visualizaciones
   - Exportar a PDF

**Nota**: El servidor FastAPI se inicia automÃ¡ticamente al ejecutar el menÃº.

### API REST

El proyecto incluye una API REST completa con FastAPI. El servidor se inicia automÃ¡ticamente con el menÃº, o puedes iniciarlo manualmente:

```bash
# OpciÃ³n 1: Script de inicio
python start.py

# OpciÃ³n 2: Comando directo
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Endpoints Principales

- `POST /api/v1/automation/unified-data`: Ejecutar proceso de automatizaciÃ³n
- `POST /api/v1/text-similarity/analyze`: Analizar similitud textual
- `GET /docs`: DocumentaciÃ³n interactiva de la API (Swagger UI)
- `GET /health`: Estado de salud del sistema

#### Ejemplo de Uso de la API

```bash
# Ejecutar proceso de automatizaciÃ³n
curl -X POST http://127.0.0.1:8000/api/v1/automation/unified-data \
     -H "Content-Type: application/json" \
     -d '{
       "base_query": "generative artificial intelligence",
       "max_articles_per_source": 100,
       "similarity_threshold": 0.75
     }'
```

### Scripts de Prueba

```bash
# Pruebas unitarias
python -m pytest tests/ -v

# Verificar salud del sistema
curl http://localhost:8000/health
```

---

## ğŸ“ Estructura del Proyecto

```
bibliometria-app/
â”œâ”€â”€ app/                              # CÃ³digo principal de la aplicaciÃ³n
â”‚   â”œâ”€â”€ api/                          # Endpoints de la API REST
â”‚   â”‚   â””â”€â”€ endpoints.py              # DefiniciÃ³n de endpoints
â”‚   â”œâ”€â”€ models/                       # Modelos de datos
â”‚   â”‚   â”œâ”€â”€ article.py                # Modelo ArticleMetadata
â”‚   â”‚   â””â”€â”€ schemas.py                # Esquemas Pydantic
â”‚   â”œâ”€â”€ services/                     # Servicios principales
â”‚   â”‚   â”œâ”€â”€ openalex_service.py       # Servicio OpenAlex
â”‚   â”‚   â”œâ”€â”€ pubmed_service.py         # Servicio PubMed
â”‚   â”‚   â”œâ”€â”€ arxiv_service.py          # Servicio ArXiv
â”‚   â”‚   â”œâ”€â”€ data_unification_service.py  # UnificaciÃ³n y detecciÃ³n de duplicados
â”‚   â”‚   â”œâ”€â”€ text_similarity_service.py    # 6 algoritmos de similitud
â”‚   â”‚   â”œâ”€â”€ word_frequency_service.py     # AnÃ¡lisis de frecuencia
â”‚   â”‚   â”œâ”€â”€ hierarchical_clustering_service.py  # Clustering jerÃ¡rquico
â”‚   â”‚   â”œâ”€â”€ visualization_service.py      # Visualizaciones
â”‚   â”‚   â””â”€â”€ geographic_service.py        # ExtracciÃ³n de datos geogrÃ¡ficos
â”‚   â”œâ”€â”€ utils/                        # Utilidades
â”‚   â”‚   â”œâ”€â”€ logger.py                 # Sistema de logging
â”‚   â”‚   â”œâ”€â”€ csv_reader.py             # Lectura de CSVs
â”‚   â”‚   â”œâ”€â”€ text_extractor.py         # ExtracciÃ³n de textos
â”‚   â”‚   â”œâ”€â”€ ollama_helper.py           # IntegraciÃ³n con Ollama
â”‚   â”‚   â”œâ”€â”€ server_helper.py           # GestiÃ³n del servidor FastAPI
â”‚   â”‚   â”œâ”€â”€ cache.py                   # Sistema de cachÃ©
â”‚   â”‚   â”œâ”€â”€ metrics.py                # MÃ©tricas de rendimiento
â”‚   â”‚   â””â”€â”€ exceptions.py              # Manejo de excepciones
â”‚   â”œâ”€â”€ config.py                      # ConfiguraciÃ³n de la aplicaciÃ³n
â”‚   â””â”€â”€ main.py                        # AplicaciÃ³n FastAPI
â”‚
â”œâ”€â”€ tests/                            # Pruebas
â”‚   â”œâ”€â”€ test_openalex_service.py      # Tests del servicio OpenAlex
â”‚   â”œâ”€â”€ test_system.py                # Tests de integraciÃ³n
â”‚   â”œâ”€â”€ test_text_similarity_service.py  # Tests de similitud textual
â”‚   â””â”€â”€ conftest.py                   # ConfiguraciÃ³n de pytest
â”‚
â”œâ”€â”€ results/                          # Archivos generados
â”‚   â”œâ”€â”€ raw_data/                     # Datos crudos por fuente
â”‚   â”œâ”€â”€ unified/                      # Archivos CSV unificados
â”‚   â”œâ”€â”€ duplicates/                   # Registro de duplicados
â”‚   â”œâ”€â”€ reports/                       # Reportes de procesamiento
â”‚   â”‚   â”œâ”€â”€ clustering/               # Dendrogramas
â”‚   â”‚   â””â”€â”€ visualizations/           # Visualizaciones y PDFs
â”‚
â”œâ”€â”€ docs/                             # DocumentaciÃ³n
â”‚   â””â”€â”€ README.md                     # Ãndice de documentaciÃ³n
â”‚
â”œâ”€â”€ scripts/                          # Scripts auxiliares
â”‚   â””â”€â”€ install_ollama.sh             # InstalaciÃ³n de Ollama
â”‚
â”œâ”€â”€ menu.py                           # MenÃº interactivo principal
â”œâ”€â”€ start.py                          # Script de inicio del servidor
â”œâ”€â”€ requirements.txt                  # Dependencias del proyecto
â”œâ”€â”€ pytest.ini                        # ConfiguraciÃ³n de pytest
â”œâ”€â”€ env.example                       # Ejemplo de configuraciÃ³n
â””â”€â”€ README.md                         # Este archivo
```

---

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

El proyecto usa un archivo `.env` para configuraciÃ³n (se crea automÃ¡ticamente). Variables principales:

```env
# API
API_HOST=0.0.0.0
API_PORT=8000

# Bases de Datos
OPENALEX_BASE_URL=https://api.openalex.org
PUBMED_BASE_URL=https://eutils.ncbi.nlm.nih.gov/entrez/eutils
ARXIV_BASE_URL=https://export.arxiv.org/api/query

# LÃ­mites
MAX_ARTICLES_DEFAULT=10
MAX_ARTICLES_LIMIT=1000

# Archivos
RESULTS_DIR=results
CSV_ENCODING=utf-8-sig

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
```

---

## ğŸ§ª Testing

```bash
# Ejecutar todas las pruebas
python -m pytest tests/ -v

# Pruebas con cobertura
python -m pytest tests/ --cov=app --cov-report=html

# Pruebas especÃ­ficas
python -m pytest tests/test_openalex_service.py -v
```

---

## ğŸ“Š Fuentes de Datos

### OpenAlex
- **Cobertura**: 200M+ trabajos acadÃ©micos globales
- **Metadatos**: Ricos y estructurados
- **API**: REST gratuita y sin lÃ­mites estrictos
- **Datos**: Citas, Open Access, afiliaciones, financiaciÃ³n

### PubMed
- **Cobertura**: Base de datos biomÃ©dica del NLM
- **Metadatos**: MeSH terms, keywords, abstracts
- **API**: Entrez/eutils REST API
- **Datos**: InformaciÃ³n mÃ©dica y biomÃ©dica especializada

### ArXiv
- **Cobertura**: Preprints de fÃ­sica, matemÃ¡ticas, ciencias de la computaciÃ³n
- **Metadatos**: CategorÃ­as, abstracts, autores
- **API**: REST API pÃºblica
- **Datos**: Preprints antes de publicaciÃ³n

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Backend
- **FastAPI**: Framework web moderno y rÃ¡pido
- **Uvicorn**: Servidor ASGI de alto rendimiento
- **Pydantic**: ValidaciÃ³n de datos y configuraciÃ³n

### Procesamiento de Datos
- **pandas**: ManipulaciÃ³n y anÃ¡lisis de datos
- **numpy**: CÃ¡lculos numÃ©ricos
- **scikit-learn**: Machine learning (TF-IDF, clustering)
- **scipy**: Algoritmos cientÃ­ficos (clustering jerÃ¡rquico)

### AnÃ¡lisis de Texto
- **NLTK**: Procesamiento de lenguaje natural
- **sentence-transformers**: Embeddings semÃ¡nticos
- **Ollama**: Modelos LLM locales

### VisualizaciÃ³n
- **matplotlib**: Visualizaciones estÃ¡ticas
- **plotly**: Visualizaciones interactivas
- **wordcloud**: Nubes de palabras
- **Pillow**: Procesamiento de imÃ¡genes

### Utilidades
- **requests**: Cliente HTTP
- **structlog**: Logging estructurado
- **python-dotenv**: GestiÃ³n de variables de entorno

---

## ğŸ“ˆ CaracterÃ­sticas Avanzadas

### DetecciÃ³n de Duplicados
- Algoritmo hÃ­brido que combina similitud de tÃ­tulo, autores, DOI y aÃ±o
- Pesos configurables para diferentes criterios
- Manejo especial de preprints vs. versiones publicadas

### ExtracciÃ³n GeogrÃ¡fica
- ExtracciÃ³n automÃ¡tica de paÃ­ses y ciudades desde afiliaciones
- NormalizaciÃ³n de nombres geogrÃ¡ficos
- Soporte para mÃºltiples formatos de afiliaciÃ³n

### Preprocesamiento de Texto
- NormalizaciÃ³n de caracteres
- EliminaciÃ³n de stopwords
- Stemming (opcional)
- Limpieza de puntuaciÃ³n y espacios

### Logging Estructurado
- Logs en formato JSON para fÃ¡cil anÃ¡lisis
- Niveles configurables
- MÃ©tricas de rendimiento integradas

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "No module named 'nltk'"
```bash
pip install nltk
python -m nltk.downloader punkt stopwords
```

### Error: "Ollama no disponible"
```bash
# Verificar que Ollama estÃ© instalado
ollama --version

# Iniciar servidor Ollama
ollama serve

# Descargar modelo
ollama pull llama3.2:3b
```

### Error: "TF-IDF fallÃ³"
- Verificar que scikit-learn estÃ© instalado: `pip install scikit-learn`
- Asegurar que los textos tengan al menos 20 caracteres

### PubMed no encuentra artÃ­culos
- Verificar que la consulta sea apropiada para PubMed
- Intentar con tÃ©rminos mÃ¡s especÃ­ficos
- Revisar logs para ver la consulta transformada

---

## ğŸ“ Notas Importantes

1. **Primera EjecuciÃ³n**: La primera vez que ejecutes el proyecto, puede tardar mÃ¡s debido a la descarga de modelos de IA (Sentence-BERT).

2. **Ollama**: El algoritmo LLM-based requiere Ollama instalado y un modelo descargado. Sin esto, solo estarÃ¡n disponibles 5 algoritmos.

3. **Memoria**: El procesamiento de grandes volÃºmenes de datos puede requerir memoria adicional. Se recomienda al menos 4GB de RAM.

4. **Internet**: Se requiere conexiÃ³n a internet para acceder a las APIs de OpenAlex, PubMed y ArXiv.

---

## ğŸ¤ ContribuciÃ³n

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'AÃ±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT.

---

## ğŸ‘¥ Autor

Desarrollado como parte de un proyecto acadÃ©mico de anÃ¡lisis bibliomÃ©trico.

---

## ğŸ”— Enlaces Ãštiles

- [OpenAlex Documentation](https://docs.openalex.org/)
- [PubMed API](https://www.ncbi.nlm.nih.gov/books/NBK25497/)
- [ArXiv API](https://arxiv.org/help/api)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Ollama Documentation](https://ollama.ai/docs)

---

## ğŸ“ Soporte

Para problemas, preguntas o sugerencias, por favor abre un issue en el repositorio del proyecto.

---

**Ãšltima actualizaciÃ³n**: Noviembre 2025
